{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports sklearn libraries\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import explained_variance_score, max_error, r2_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Imports python libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Imports for activity cliffs\n",
    "from rdkit.Chem import Draw # for molecule depiction\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit import Chem\n",
    "\n",
    "from activity_cliffs import cliffs_finder # for the training set\n",
    "from activity_cliffs import cliffs_finder_test # for the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading specific dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet was created in case a user wants to load data from one specific CSV file. Used for test purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "train_data = pd.read_csv('../CHEMBL239_dataset_train.csv')\n",
    "test_data = pd.read_csv('../CHEMBL239_dataset_test.csv')\n",
    "\n",
    "# Get x and y values\n",
    "y_train = train_data.loc[:, 'exp_mean']\n",
    "y_test = test_data.loc[:, 'exp_mean']\n",
    "X_train = train_data.loc[:, 'Bit 1' : 'Bit 1024']\n",
    "X_test = test_data.loc[:, 'Bit 1' : 'Bit 1024']\n",
    "\n",
    "training_dataset = X_train, y_train\n",
    "testing_dataset = X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading all datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet loads all data used for this specific Bachelor End Project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c261703d7de5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# For loop that runs all models on all files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mchem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdescriptor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdescriptors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "chems = ['239', '4005', '224']\n",
    "descriptors = ['CATS', 'CDKmolprop', 'constitutional', 'WHIM', 'Druglike', 'MorganFP']\n",
    "models = ['knn', 'rf', 'svr']\n",
    "\n",
    "# For loop that runs all models on all files\n",
    "for chem in tqdm(chems):\n",
    "    print(chem)\n",
    "    for descriptor in descriptors:\n",
    "        print(descriptor)\n",
    "        train_data = pd.read_csv(f'../descriptors_data/CHEMBL{chem}_train_{descriptor}.csv')\n",
    "        test_data = pd.read_csv(f'../descriptors_data/CHEMBL{chem}_test_{descriptor}.csv')\n",
    "        \n",
    "        # Get x and y values\n",
    "        y_train = train_data.loc[:, 'Y'].values.astype(np.float)\n",
    "        y_test = test_data.loc[:, 'Y'].values.astype(np.float)\n",
    "        x_train = train_data.iloc[:, 3:].values.astype(np.float)\n",
    "        x_test = test_data.iloc[:, 3:].values.astype(np.float)\n",
    "        \n",
    "        # Scaling the x-values of the training and testing set\n",
    "        scaler = StandardScaler().fit(x_train)\n",
    "    \n",
    "        x_train_scaled = scaler.transform(x_train)\n",
    "        x_test_scaled = scaler.transform(x_test)\n",
    "        \n",
    "        # Create final dataset for the models\n",
    "        training_dataset = x_train_scaled, y_train\n",
    "        testing_dataset = x_test_scaled, y_test\n",
    "        \n",
    "        # Loop over all model types\n",
    "        for model in models:\n",
    "            print(model)\n",
    "            model_selection(model, training_dataset, testing_dataset, chem, descriptor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K - Nearest Neighbors Model\n",
    "def KNN_model(training_dataset, model, chem, descriptor, testing_dataset, distance_type=\"euclidean\"):\n",
    "    \n",
    "    #Create training and testing data\n",
    "    x_train_scaled, y_train = training_dataset\n",
    "    x_test_scaled, y_test = testing_dataset\n",
    "    \n",
    "    # Lists for the visualize function\n",
    "    list_y_pred_train = []\n",
    "    list_y_pred = []\n",
    "    list_y_actual = []\n",
    "    list_y_diff = []\n",
    "    \n",
    "    # Metrics for all number of neighbors\n",
    "    metrics_KNN_all = []\n",
    "    \n",
    "    # Set the parameters to check with GridSearch\n",
    "    param = {'n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}\n",
    "    \n",
    "    # GridSearch on KNN\n",
    "    knn_model = GridSearchCV(KNeighborsRegressor(metric=distance_type), param, cv=5, n_jobs=-1, verbose=3, return_train_score=True).fit(x_train_scaled, y_train)\n",
    "    \n",
    "    # Store the best n_neighbors parameter\n",
    "    best_param = knn_model.best_params_['n_neighbors']\n",
    "    \n",
    "    # Print the best n_neighbors parameter\n",
    "    print('The best value for the n_neighbors parameter in the KNN model is:', best_param)\n",
    "    #Make KNN with best n_neighbors parameter\n",
    "    \n",
    "    best_knn_model = KNeighborsRegressor(n_neighbors=best_param, n_jobs=-1, metric=distance_type).fit(x_train_scaled, y_train)\n",
    "    # Make prediction\n",
    "    \n",
    "    y_pred_train=knn_model.predict(x_train_scaled)\n",
    "    y_pred_knn=knn_model.predict(x_test_scaled)\n",
    "        \n",
    "    # Compute the metrics and store it in the total list of metrics\n",
    "    metrics_KNN = compute_metric(y_test, y_pred_knn, y_pred_train, y_train, model, chem, descriptor)\n",
    "    \n",
    "    for item in y_pred_train:\n",
    "        list_y_pred_train.append(item)        \n",
    "    for item in y_pred_knn:\n",
    "        list_y_pred.append(item)\n",
    "    for item in y_test:\n",
    "        list_y_actual.append(item)\n",
    "    for index in range(0, len(y_test)):\n",
    "        list_y_diff.append(list_y_pred[index] - list_y_actual[index])\n",
    "    \n",
    "    # Create plots of the data\n",
    "    visualize('knn', chem, descriptor, list_y_actual, list_y_pred, list_y_diff)\n",
    "        \n",
    "    return metrics_KNN\n",
    "\n",
    "\n",
    "# Support Vector Regression Model\n",
    "def SVR_model(training_dataset, model, chem, descriptor, testing_dataset):\n",
    "    \n",
    "    # Create training and testing data\n",
    "    x_train_scaled, y_train = training_dataset\n",
    "    x_test_scaled, y_test = testing_dataset\n",
    "    \n",
    "    # Lists for the visualize function\n",
    "    list_y_pred_train = []\n",
    "    list_y_pred = []\n",
    "    list_y_actual = []\n",
    "    list_y_diff = []\n",
    "    \n",
    "    # Set the parameters to check with GridSearch\n",
    "    param = {'kernel': ['rbf']}\n",
    "\n",
    "    # GridSearch on SVR\n",
    "    svr_model = GridSearchCV(SVR(), param, cv=5, n_jobs=-1, verbose=2).fit(x_train_scaled, y_train)\n",
    "    \n",
    "    # Store the best kernel parameter\n",
    "    best_param = svr_model.best_params_['kernel']\n",
    "    \n",
    "    # Print the best kernel parameter\n",
    "    print('The best value for the kernel parameter in the SVR model is:', best_param)\n",
    "\n",
    "    # Make SVR with best kernel parameter\n",
    "    best_svr_model = SVR(kernel = 'rbf').fit(x_train_scaled, y_train)\n",
    "\n",
    "    # Make prediction\n",
    "    y_pred_train= best_svr_model.predict(x_train_scaled)\n",
    "    y_pred_svr = best_svr_model.predict(x_test_scaled)\n",
    "    \n",
    "    # Compute the metrics and store it in the total list of metrics\n",
    "    metrics_SVR = compute_metric(y_test, y_pred_svr, y_pred_train, y_train, model, chem, descriptor)\n",
    "    \n",
    "    for item in y_pred_train:\n",
    "        list_y_pred_train.append(item)\n",
    "    for item in y_pred_svr:\n",
    "        list_y_pred.append(item)\n",
    "\n",
    "    for item in y_test:\n",
    "        list_y_actual.append(item)\n",
    "\n",
    "    for index in range(0, len(y_test)):\n",
    "        list_y_diff.append(list_y_pred[index] - list_y_actual[index])\n",
    "        \n",
    "    # Create plots of the data\n",
    "    visualize('svr', chem, descriptor, list_y_actual, list_y_pred, list_y_diff)\n",
    "        \n",
    "    return metrics_SVR\n",
    "\n",
    "    \n",
    "# Random Forest model \n",
    "def RF_model(training_dataset, model, chem, descriptor, testing_dataset):\n",
    "    \n",
    "    # Create training and testing data\n",
    "    x_train_scaled, y_train = training_dataset\n",
    "    x_test_scaled, y_test = testing_dataset\n",
    "    \n",
    "    # Lists for the visualize function\n",
    "    list_y_pred_train = []\n",
    "    list_y_pred = []\n",
    "    list_y_actual = []\n",
    "    list_y_diff = []\n",
    "    \n",
    "    #Set the parameters to check with GridSearch\n",
    "    param = {'n_estimators': [50, 100, 500, 1000]}\n",
    "\n",
    "    #GridSearch on RandomForestRegressor\n",
    "    rf_model = GridSearchCV(RandomForestRegressor(), param, cv=5, n_jobs=-1, verbose=0).fit(x_train_scaled, y_train)\n",
    "    \n",
    "    # Store the best n_estimator parameter\n",
    "    best_param = rf_model.best_params_['n_estimators']\n",
    "    \n",
    "    # Print the best n_estimator parameter\n",
    "    print('The best value for the n_estimator parameter in the RF model is:', best_param)\n",
    "\n",
    "    # Make randomforest with best value for n_estimator\n",
    "    best_rf_model = RandomForestRegressor(n_estimators = best_param).fit(x_train_scaled, y_train)\n",
    "\n",
    "    # Make prediction\n",
    "    y_pred_train=best_rf_model.predict(x_train_scaled)\n",
    "    y_pred_rf=best_rf_model.predict(x_test_scaled)\n",
    "\n",
    "    # Compute regression metrics\n",
    "    metrics_RF = compute_metric(y_test, y_pred_rf, y_pred_train, y_train, model, chem, descriptor)\n",
    "    \n",
    "    for item in y_pred_train:\n",
    "        list_y_pred_train.append(item)\n",
    "    for item in y_pred_rf:\n",
    "        list_y_pred.append(item)\n",
    "\n",
    "    for item in y_test:\n",
    "        list_y_actual.append(item)\n",
    "\n",
    "    for index in range(0, len(y_test)):\n",
    "        list_y_diff.append(list_y_pred[index] - list_y_actual[index])\n",
    "    \n",
    "    # Create plots of the data\n",
    "    visualize('rf', chem, descriptor, list_y_actual, list_y_pred, list_y_diff)\n",
    "        \n",
    "    return metrics_RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the RMSEP metric\n",
    "def RMSEP_train(y_true, y_pred, model, chem, descriptor):\n",
    "    '''\n",
    "    Compute Root Mean Square Percentage Error between two arrays.\n",
    "    '''\n",
    "    \n",
    "    # Compute the actual metric\n",
    "    RMSEP = np.sqrt(np.sum(np.square(y_true - y_pred))/len(y_test))\n",
    " \n",
    "    return RMSEP\n",
    "\n",
    "# Function to compute the Q2F3 metric\n",
    "def RMSEP_test(y_true, y_pred, model, chem, descriptor):\n",
    "    '''\n",
    "    Compute Root Mean Square Percentage Error between two arrays.\n",
    "    '''\n",
    "    \n",
    "    # Compute the actual metric\n",
    "    RMSEP = np.sqrt(np.sum(np.square(y_true - y_pred))/len(y_test))\n",
    " \n",
    "    return RMSEP\n",
    "    \n",
    "\n",
    "# Compute all metrics\n",
    "def compute_metric(y_test, y_pred, y_train, y_pred_train, model, chem, descriptor):\n",
    "    \n",
    "    # Make lists for metrics\n",
    "    r2_values = []\n",
    "    RMSEP_train_values = []\n",
    "    RMSEP_test_values = []\n",
    "    \n",
    "    # Compute regression metrics\n",
    "    r2_values.append(r2_score(y_train, y_pred_train))\n",
    "    RMSEP_train_values.append(RMSEP_train(y_train, y_pred_train, model, chem, descriptor))\n",
    "    RMSEP_test_values.append(RMSEP_test(y_test, y_pred, model, chem, descriptor))\n",
    "    \n",
    "    return r2_values, RMSEP_train_values, RMSEP_test_values\n",
    "\n",
    "def save_metrics(variables, model, chem, descriptor):\n",
    "    # Add setting to the output list\n",
    "    output = [model, chem, descriptor]\n",
    "    \n",
    "    # Loop over variables and store it into output list \n",
    "    for i in range(len(variables)):\n",
    "        output.append(variables[i])\n",
    "    \n",
    "    # Create a pandas dataframe of variables and transpose it\n",
    "    DF = pd.DataFrame(output).T\n",
    "    \n",
    "    # Custom column names\n",
    "    column_names = ['model', 'chem', 'descriptor', 'r2_value', 'RMSEP_train', 'RMSEP_test']\n",
    "    \n",
    "    # Store the variables as CSV to disk with custom column names\n",
    "    DF.to_csv(f'variables/{model}_{chem}_{descriptor}_variables_withcliffs.csv\", header=column_names, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection function\n",
    "def model_selection(model, training_dataset, testing_dataset, chem=None, descriptor=None):\n",
    "    if model == 'knn':\n",
    "        \n",
    "        # Checks if the descriptor is MorganFP, applies jaccard when true as distance metric. \n",
    "        # Otherwise uses default euclidean distance\n",
    "        if descriptor == 'MorganFP':\n",
    "            distance_type = \"jaccard\"\n",
    "        else:\n",
    "            distance_type= \"euclidean\"\n",
    "            \n",
    "        variables = KNN_model(training_dataset, model, chem, descriptor, testing_dataset, distance_type)\n",
    "        save_metrics(variables, model, chem, descriptor)\n",
    "        \n",
    "    elif model == 'rf':\n",
    "        variables = RF_model(training_dataset, model, chem, descriptor, testing_dataset)\n",
    "        save_metrics(variables, model, chem, descriptor)\n",
    "        \n",
    "    elif model == 'svr':\n",
    "        variables = SVR_model(training_dataset, model, chem, descriptor, testing_dataset)\n",
    "        save_metrics(variables, model, chem, descriptor)\n",
    "        \n",
    "    else:\n",
    "        print('Error! This is not a valid model name. Select knn, rf or svr.')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data\n",
    "def visualize(model, chem, descriptor, y_actual, y_pred, y_diff):\n",
    "    \n",
    "    # Make plots\n",
    "    from textwrap import wrap\n",
    "    fig, axes = plt.subplots(1, 2,  figsize=(15, 7))\n",
    "    #plt.subplots_adjust(top=0.7)\n",
    "    axes[0].plot(y_actual, lw=2, color=\"red\", label=\"actual y value\")\n",
    "    axes[0].set_ylim(ymin=3)\n",
    "    axes[0].plot(y_pred, lw=2, color=\"blue\", label=\"predicted y value\")\n",
    "    axes[0].legend()\n",
    "    axes[0].set_title(\"\\n\".join(wrap(f'Plot of the actual and predicted potency values for {model} model on ChEMBL{chem} with cliffs with descriptor type {descriptor}.')), size = 14)\n",
    "    axes[0].set_xlabel('Test set molecule index', size = 14)\n",
    "    axes[0].set_ylabel('Potency values', size = 14)\n",
    "    \n",
    "\n",
    "    axes[1].scatter(y_pred, y_actual, lw=2, color=\"purple\")\n",
    "    axes[1].set_ylim(ymin=4, ymax=12)\n",
    "    axes[1].set_xlim(xmin=4, xmax=12)\n",
    "    axes[1].plot([0, 1], [0, 1], transform=axes[1].transAxes, color=\"blue\")\n",
    "    axes[1].set_title(\"\\n\".join(wrap(f'Scatterplot of the actual and predicted potency values {model} model on ChEMBL{chem} with cliffs with descriptor type {descriptor}.')), size = 14)\n",
    "    axes[1].set_xlabel('Actual potency values', size = 14)\n",
    "    axes[1].set_ylabel('Predicted potency values', size = 14)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig.savefig(f'figures/{model}_{chem}_{descriptor}_withcliffs.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a specific model on a specific training and testing set. The chem and descriptor parameters can be null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select knn, rf or svr\n",
    "model_selection('knn', training_dataset, testing_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
